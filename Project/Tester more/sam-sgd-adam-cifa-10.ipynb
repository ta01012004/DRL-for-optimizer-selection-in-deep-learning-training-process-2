{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math, random, time, copy, builtins, functools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions as tdist\nfrom torch import optim\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score\nfrom torchvision.datasets import CIFAR10\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.models as models\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:03.843357Z","iopub.execute_input":"2025-07-03T18:09:03.844483Z","iopub.status.idle":"2025-07-03T18:09:03.850785Z","shell.execute_reply.started":"2025-07-03T18:09:03.844446Z","shell.execute_reply":"2025-07-03T18:09:03.849939Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print = functools.partial(builtins.print, flush=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:03.851951Z","iopub.execute_input":"2025-07-03T18:09:03.852288Z","iopub.status.idle":"2025-07-03T18:09:03.880442Z","shell.execute_reply.started":"2025-07-03T18:09:03.852262Z","shell.execute_reply":"2025-07-03T18:09:03.879631Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = 42\n# Cố định seed để tái hiện kết quả\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:03.881720Z","iopub.execute_input":"2025-07-03T18:09:03.882012Z","iopub.status.idle":"2025-07-03T18:09:03.900789Z","shell.execute_reply.started":"2025-07-03T18:09:03.881988Z","shell.execute_reply":"2025-07-03T18:09:03.900093Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## ---------------------------- 1. Config ----------------------------------\n","metadata":{}},{"cell_type":"code","source":"ROLLOUT_STEPS = 4        # số bước (epoch) trước mỗi lần cập nhật PPO\nGAMMA = 0.99             # hệ số chiết khấu cho reward\nLAMBDA_GAE = 0.95        # hệ số lambda cho GAE (Generalized Advantage Estimation)\nPPO_EPOCHS = 4           # số lần lặp cập nhật PPO cho mỗi batch dữ liệu\nCLIP_EPS = 0.1           # phạm vi clipping cho PPO\nENT_COEF = 5e-3          # hệ số trọng số entropy (khuyến khích khám phá)\nLR_POLICY = 3e-4         # learning rate cho network policy (diễn viên)\nLR_VALUE  = 1e-3         # learning rate cho network value (phê bình)\nSTATE_DIM = 7            # kích thước vector trạng thái (định nghĩa bên dưới)\nACTION_DIM = 2           # số hành động (0: SGD, 1: Adam, 2: SAM)\n# Hệ số tính reward\nW_ACC = 100.0            # trọng số cho thay đổi accuracy\nW_LOSS = 1.0             # trọng số cho thay đổi loss\nW_TIME = 0.03             # trọng số phạt thời gian (epoch lâu sẽ bị trừ reward)\nFINAL_BONUS_W = 50.0     # trọng số thưởng thêm cuối cùng dựa trên độ chính xác validation\nLOSS_SCALE = 2.0         # hệ số scale cho loss (để giá trị loss và acc ở cùng mức độ)\nWEIGHT_DECAY = 5e-4      # weight decay áp dụng cho các optimizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:03.902112Z","iopub.execute_input":"2025-07-03T18:09:03.902359Z","iopub.status.idle":"2025-07-03T18:09:03.918058Z","shell.execute_reply.started":"2025-07-03T18:09:03.902342Z","shell.execute_reply":"2025-07-03T18:09:03.917368Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"SAM\n","metadata":{}},{"cell_type":"code","source":"class SAM(optim.Optimizer):\n    \"\"\"Optimizer SAM: Sharpness-Aware Minimization (với hai bước cập nhật).\"\"\"\n    def __init__(self, params, base_optimizer, rho=0.05, grad_norm_eps=1e-12, **kwargs):\n        # base_optimizer: lớp optimizer cơ sở (vd: optim.SGD)\n        super().__init__(params, dict(rho=rho, grad_norm_eps=grad_norm_eps, **kwargs))\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        # Bước 1: cập nhật tạm thời theo hướng tăng độ sắc nét (maximize loss)\n        grad_norm = torch.norm(torch.stack([\n            p.grad.norm() for group in self.param_groups\n            for p in group['params'] if p.grad is not None\n        ]))\n        if grad_norm < self.defaults['grad_norm_eps']:\n            return\n        scale = self.defaults['rho'] / (grad_norm + 1e-12)\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None: \n                    continue\n                # Nhảy một bước nhỏ theo hướng gradient (tăng weight tạm thời)\n                e_w = p.grad * scale\n                p.add_(e_w)\n                # Lưu e_w để dùng cho bước thứ hai\n                self.state[p]['e_w'] = e_w.detach()\n        if zero_grad:\n            self.zero_grad()\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        # Bước 2: quay về tọa độ ban đầu và tối ưu một bước theo optimizer cơ sở\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                # Quay lại trọng số ban đầu\n                p.sub_(self.state[p]['e_w'])\n        # Cập nhật trọng số theo optimizer cơ sở (SGD)\n        self.base_optimizer.step()\n        if zero_grad:\n            self.zero_grad()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:03.918990Z","iopub.execute_input":"2025-07-03T18:09:03.919195Z","iopub.status.idle":"2025-07-03T18:09:03.932617Z","shell.execute_reply.started":"2025-07-03T18:09:03.919179Z","shell.execute_reply":"2025-07-03T18:09:03.931883Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# --------------------- 3. Data -----------------------------------------\n","metadata":{}},{"cell_type":"code","source":"def get_resnet18_scratch(num_classes=10):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n# ==== 4) Tải dữ liệu CIFAR-10 (tách train/val/test) ====\nfrom torchvision.datasets import FashionMNIST\n\nfrom torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.transforms as T\nimport torch\n\ndef load_fashion_mnist(batch_train=128, batch_eval=256, seed=42):\n    \"\"\"\n    Trả về dl_train, dl_val, dl_test cho FashionMNIST,\n    với ảnh được chuyển sang 3 channels để dùng ResNet-18 mặc định.\n    \"\"\"\n    # Transform cho train: chuyển grayscale→3 channels rồi augment\n    transform_train = T.Compose([\n        T.Grayscale(num_output_channels=3),     # 1→3 channels\n        T.RandomHorizontalFlip(),\n        T.RandomCrop(28, padding=4),\n        T.ToTensor(),\n        T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n    ])\n    # Transform cho eval: chỉ chuyển và chuẩn hóa\n    transform_eval = T.Compose([\n        T.Grayscale(num_output_channels=3),\n        T.ToTensor(),\n        T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n    ])\n\n    # Tải dataset\n    full_train = FashionMNIST(\n        root='./data',\n        train=True,\n        download=True,\n        transform=transform_train\n    )\n    test_set = FashionMNIST(\n        root='./data',\n        train=False,\n        download=True,\n        transform=transform_eval\n    )\n\n    # Tách validation từ train (5.000 mẫu)\n    val_size = 5000\n    train_size = len(full_train) - val_size\n    train_set, val_set = random_split(\n        full_train,\n        [train_size, val_size],\n        generator=torch.Generator().manual_seed(seed)\n    )\n\n    # Tạo DataLoader\n    dl_train = DataLoader(\n        train_set,\n        batch_size=batch_train,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    dl_val = DataLoader(\n        val_set,\n        batch_size=batch_eval,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    dl_test = DataLoader(\n        test_set,\n        batch_size=batch_eval,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    print(f\"Dataset FashionMNIST → Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}\")\n    return dl_train, dl_val, dl_test\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:04.001472Z","iopub.execute_input":"2025-07-03T18:09:04.002163Z","iopub.status.idle":"2025-07-03T18:09:04.011979Z","shell.execute_reply.started":"2025-07-03T18:09:04.002129Z","shell.execute_reply":"2025-07-03T18:09:04.011195Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# --------------------- 4. Model & Optimizers ---------------------------\n","metadata":{}},{"cell_type":"code","source":"class PolicyNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(STATE_DIM, 64), nn.Tanh(),\n            nn.Linear(64, 64), nn.Tanh(),\n            nn.Linear(64, ACTION_DIM)\n        )\n    def forward(self, x):\n        return self.net(x)  # trả về logits (chưa softmax) cho mỗi hành động\n\nclass ValueNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(STATE_DIM, 64), nn.Tanh(),\n            nn.Linear(64, 64), nn.Tanh(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, x):\n        # Trả về giá trị (scalar) đại diện cho Value function V(s)\n        return self.net(x).squeeze(-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:04.013417Z","iopub.execute_input":"2025-07-03T18:09:04.013625Z","iopub.status.idle":"2025-07-03T18:09:04.026845Z","shell.execute_reply.started":"2025-07-03T18:09:04.013610Z","shell.execute_reply":"2025-07-03T18:09:04.026162Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class RolloutBuffer:\n    def __init__(self):\n        self.clear()\n    def store(self, state, action, log_prob, value, reward, done):\n        # Lưu một transition (state, action, log_prob, value, reward, done) vào buffer\n        self.states.append(state.detach())\n        self.actions.append(action.detach())\n        self.logprobs.append(log_prob.detach())\n        self.values.append(value.detach())\n        # Đảm bảo reward là tensor float32 trên DEVICE\n        if not isinstance(reward, torch.Tensor):\n            reward = torch.tensor(reward, dtype=torch.float32, device=DEVICE)\n        self.rewards.append(reward)\n        # done flag (float32) — 1.0 nếu xong episode, 0.0 nếu chưa\n        self.dones.append(torch.tensor(float(done), device=DEVICE))\n    def compute(self, next_value):\n        # Tính toán lợi thế (advantage) GAE và return (target) cho từng bước sau khi rollout kết thúc\n        self.advs = []\n        self.rets = []\n        adv = 0.0  # biến lưu trữ lợi thế tích lũy\n        for i in reversed(range(len(self.rewards))):\n            mask = 1.0 - self.dones[i]  # 0 nếu done, 1 nếu chưa xong (tiếp tục)\n            # delta = phần tử TD-error cho bước i\n            delta = self.rewards[i] + GAMMA * next_value * mask - self.values[i]\n            adv = delta + GAMMA * LAMBDA_GAE * mask * adv  # GAE: tích lũy lợi thế có chiết khấu\n            self.advs.insert(0, adv)                      # thêm vào đầu danh sách (do duyệt ngược)\n            self.rets.insert(0, adv + self.values[i])     # return = advantage + value (Q = V + A)\n            next_value = self.values[i]                   # cập nhật next_value cho bước trước đó\n    def as_tensors(self):\n        # Chuyển các danh sách lưu trữ sang tensor và đưa lên DEVICE\n        fixed_states = [(s.unsqueeze(0) if s.dim() == 0 else s) for s in self.states]\n        states   = torch.stack(fixed_states).to(DEVICE)\n        actions  = torch.stack(self.actions).to(DEVICE)\n        logprobs = torch.stack(self.logprobs).to(DEVICE)\n        advs     = torch.stack(self.advs).to(DEVICE)\n        rets     = torch.stack(self.rets).to(DEVICE)\n        return states, actions, logprobs, advs, rets\n    def clear(self):\n        # Xóa sạch bộ nhớ\n        self.states   = []\n        self.actions  = []\n        self.logprobs = []\n        self.values   = []\n        self.rewards  = []\n        self.dones    = []\n        self.advs     = []\n        self.rets     = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:04.027765Z","iopub.execute_input":"2025-07-03T18:09:04.028037Z","iopub.status.idle":"2025-07-03T18:09:04.039528Z","shell.execute_reply.started":"2025-07-03T18:09:04.028012Z","shell.execute_reply":"2025-07-03T18:09:04.038689Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"ID2OPT = ['sgd', 'sam']  # ánh xạ id hành động sang tên optimizer\ndef make_optimizer(name, model, lr):\n    if name == 'sgd':\n        return optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=WEIGHT_DECAY)\n    elif name == 'sam':\n        # SAM sử dụng SGD làm optimizer cơ sở\n        return SAM(model.parameters(), base_optimizer=optim.SGD, lr=lr, momentum=0.9, weight_decay=WEIGHT_DECAY)\n    else:\n        raise ValueError(f\"Unknown optimizer name: {name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:04.040519Z","iopub.execute_input":"2025-07-03T18:09:04.040774Z","iopub.status.idle":"2025-07-03T18:09:04.051537Z","shell.execute_reply.started":"2025-07-03T18:09:04.040741Z","shell.execute_reply":"2025-07-03T18:09:04.050702Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def compute_metrics(model, loader):\n    model.eval()\n    loss_fn = nn.CrossEntropyLoss()\n    total_loss = 0.0\n    preds_list = []\n    labels_list = []\n    n_samples = 0\n    for X, y in loader:\n        X, y = X.to(DEVICE), y.to(DEVICE)\n        logits = model(X)\n        loss = loss_fn(logits, y)\n        # Cộng dồn loss và gom nhãn dự đoán/thực tế để tính độ chính xác, precision, F1\n        total_loss += loss.item() * y.size(0)\n        preds_list.append(logits.argmax(dim=1).cpu())\n        labels_list.append(y.cpu())\n        n_samples += y.size(0)\n    # Kết hợp các batch lại\n    preds  = torch.cat(preds_list)\n    labels = torch.cat(labels_list)\n    # Tính các chỉ số\n    acc  = accuracy_score(labels, preds)\n    prec = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n    f1   = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n    avg_loss = total_loss / n_samples\n    return acc, prec, f1, avg_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:04.052952Z","iopub.execute_input":"2025-07-03T18:09:04.053163Z","iopub.status.idle":"2025-07-03T18:09:04.064966Z","shell.execute_reply.started":"2025-07-03T18:09:04.053148Z","shell.execute_reply":"2025-07-03T18:09:04.064458Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class OptimizerRLTrainer:\n    def __init__(self, model, dl_train, dl_val, dl_test, n_epochs=150, patience=10):\n        self.model = model.to(DEVICE)\n        self.dl_train = dl_train\n        self.dl_val   = dl_val\n        self.dl_test  = dl_test\n        self.n_epochs = n_epochs\n        self.patience = patience\n        # Khởi tạo network cho Policy (chọn hành động) và Value (ước lượng V)\n        self.policy = PolicyNet().to(DEVICE)\n        self.value  = ValueNet().to(DEVICE)\n        self.opt_policy = optim.Adam(self.policy.parameters(), lr=LR_POLICY)\n        self.opt_value  = optim.Adam(self.value.parameters(),  lr=LR_VALUE)\n        # Lịch sử các chỉ số theo từng epoch\n        self.hist = {k: [] for k in [\n            \"train_acc\", \"val_acc\", \"test_acc\",\n            \"train_prec\", \"val_prec\", \"test_prec\",\n            \"train_f1\", \"val_f1\", \"test_f1\",\n            \"train_loss\", \"val_loss\", \"test_loss\",\n            \"reward\", \"action\", \"time\"\n        ]}\n        # Theo dõi mô hình có độ chính xác val tốt nhất (để so sánh)\n        self.best_val = 0.0\n        self.best_epoch = 0\n        self.best_test_acc = 0.0\n        self.best_test_loss = 0.0\n        self.best_state = None  # lưu state_dict của model tốt nhất\n\n    def _ppo_update(self, buffer):\n        states, actions, old_logprobs, advs, rets = buffer.as_tensors()\n\n    # Chuẩn hoá advantage: dùng unbiased=False và chỉ khi có >1 phần tử\n        if advs.numel() > 1:\n            std = advs.std(unbiased=False)\n            advs = (advs - advs.mean()) / (std + 1e-8)\n        else:\n            advs = torch.zeros_like(advs)\n\n    # PPO update lặp PPO_EPOCHS lần\n        for _ in range(PPO_EPOCHS):\n            dist = tdist.Categorical(logits=self.policy(states))\n            new_logprob = dist.log_prob(actions)\n            ratio = torch.exp(new_logprob - old_logprobs)\n\n            surr1 = ratio * advs\n            surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs\n            policy_loss = -(torch.min(surr1, surr2) + ENT_COEF * dist.entropy()).mean()\n\n            # update policy network\n            self.opt_policy.zero_grad()\n            policy_loss.backward()\n            self.opt_policy.step()\n\n            # update value network\n            value_pred = self.value(states)\n            value_loss = F.mse_loss(value_pred, rets)\n            self.opt_value.zero_grad()\n            value_loss.backward()\n            self.opt_value.step()\n\n            \n\n    def fit(self):\n        loss_fn = nn.CrossEntropyLoss()\n        buffer = RolloutBuffer()\n        # Các biến trạng thái ban đầu (cho epoch 0)\n        prev_val_acc = 0.0\n        prev_val_loss = 0.0\n        delta_acc_prev = 0.0\n        delta_loss_prev = 0.0\n        prev_action = 0           # giả định optimizer trước đó là SGD (id=0) lúc bắt đầu\n        patience_counter = 0      # đếm số epoch liên tiếp không cải thiện val_acc\n\n        for ep in range(self.n_epochs):\n            t0 = time.time()\n            # 1) Tạo vector trạng thái cho epoch hiện tại\n            state = torch.tensor([\n                prev_val_acc,\n                prev_val_loss / LOSS_SCALE,\n                delta_acc_prev,\n                delta_loss_prev,\n                ep / self.n_epochs,\n                *[int(prev_action == i) for i in range(ACTION_DIM)]\n            ], dtype=torch.float32, device=DEVICE)\n            # Vector trạng thái gồm:\n            # [val_acc (trước đó), val_loss (trước đó, đã scale), \n            #  delta_val_acc (trước đó), delta_val_loss (trước đó),\n            #  tỉ lệ epoch hiện tại, one-hot hành động trước đó (3 phần tử)]\n\n            # 2) Policy chọn một optimizer (hành động)\n            dist = tdist.Categorical(logits=self.policy(state))\n            action = dist.sample()                # chọn hành động (0, 1 hoặc 2)\n            log_prob = dist.log_prob(action)      # log-probability của hành động đã chọn\n            opt_idx = action.item()\n            opt_name = ID2OPT[opt_idx]\n            # Chọn learning rate cho optimizer: Adam dùng lr=1e-3, còn lại lr=1e-2\n            lr = 1e-3 if opt_name == \"adam\" else 1e-2\n            optimizer = make_optimizer(opt_name, self.model, lr)\n\n            # 3) Huấn luyện mô hình trong 1 epoch với optimizer đã chọn\n            self.model.train()\n            total_loss = 0.0\n            total_correct = 0\n            total_samples = 0\n            train_preds, train_labels = [], []\n            for X, y in self.dl_train:\n                X, y = X.to(DEVICE), y.to(DEVICE)\n                optimizer.zero_grad()\n                logits = self.model(X)\n                loss   = loss_fn(logits, y)\n                if opt_name == \"sam\":\n                    # Nếu optimizer là SAM: thực hiện hai bước cập nhật\n                    loss.backward()\n                    optimizer.first_step(zero_grad=True)   # bước 1: di chuyển đến điểm sắc nét\n                    # Tính loss tại điểm sắc nét (model đã thay đổi sau first_step)\n                    logits_sam = self.model(X)\n                    loss_sam = loss_fn(logits_sam, y)\n                    loss_sam.backward()\n                    optimizer.second_step(zero_grad=True)  # bước 2: cập nhật trọng số theo SGD\n                else:\n                    # Optimizer thường (SGD hoặc Adam): chỉ một bước\n                    loss.backward()\n                    optimizer.step()\n                # Cập nhật thống kê train\n                total_loss    += loss.item() * y.size(0)\n                total_correct += (logits.argmax(dim=1) == y).sum().item()\n                total_samples += y.size(0)\n                train_preds.append(logits.argmax(dim=1).cpu())\n                train_labels.append(y.cpu())\n            # Tính các chỉ số train sau epoch\n            train_preds  = torch.cat(train_preds)\n            train_labels = torch.cat(train_labels)\n            train_acc  = total_correct / total_samples\n            train_prec = precision_score(train_labels, train_preds, average=\"weighted\", zero_division=0)\n            train_f1   = f1_score(train_labels, train_preds, average=\"weighted\", zero_division=0)\n            train_loss = total_loss / total_samples\n\n            # 4) Đánh giá trên tập validation và test\n            val_acc,  val_prec,  val_f1,  val_loss  = compute_metrics(self.model, self.dl_val)\n            test_acc, test_prec, test_f1, test_loss = compute_metrics(self.model, self.dl_test)\n\n            # 5) Kiểm tra cải thiện trên validation để lưu model tốt nhất (checkpoint)\n            if val_acc > self.best_val:\n                # Nếu val_acc tốt hơn trước, cập nhật thông tin model tốt nhất\n                self.best_val = val_acc\n                self.best_epoch = ep + 1\n                self.best_test_acc = test_acc\n                self.best_test_loss = test_loss\n                self.best_state = copy.deepcopy(self.model.state_dict())\n                patience_counter = 0\n            else:\n                patience_counter += 1\n\n            # 6) Tính reward dựa trên thay đổi độ chính xác/loss và thời gian huấn luyện\n            delta_acc  = val_acc - prev_val_acc       # thay đổi Val Acc so với epoch trước\n            delta_loss = prev_val_loss - val_loss     # giảm bớt Val Loss so với epoch trước\n            elapsed    = time.time() - t0             # thời gian huấn luyện epoch (giây)\n            # Reward: tăng nếu val_acc tăng hoặc val_loss giảm, trừ đi nếu tốn nhiều thời gian\n            reward = W_ACC * delta_acc + W_LOSS * delta_loss - W_TIME * elapsed\n            # Nếu là epoch cuối (kết thúc episode), thưởng thêm dựa trên val_acc hiện tại\n            if ep == self.n_epochs - 1:\n                reward += FINAL_BONUS_W * val_acc\n\n            # Lưu transition vào buffer\n            done = (ep == self.n_epochs - 1)  # True nếu là epoch cuối\n            buffer.store(state, action, log_prob, self.value(state).detach(), reward, done)\n            # Nếu đã thu thập đủ ROLLOUT_STEPS bước, hoặc đã kết thúc episode, thì cập nhật PPO\n            if (ep + 1) % ROLLOUT_STEPS == 0 or done:\n                next_val = torch.tensor(0.0, device=DEVICE)\n                if not done:\n                    # Nếu chưa done, ước lượng giá trị của state kế tiếp để làm next_value (bootstrap)\n                    next_val = self.value(state).detach()\n                buffer.compute(next_val)\n                self._ppo_update(buffer)\n                buffer.clear()\n\n            # 7) Lưu lại lịch sử các chỉ số cho epoch này\n            self.hist[\"train_acc\"].append(train_acc);   self.hist[\"val_acc\"].append(val_acc);   self.hist[\"test_acc\"].append(test_acc)\n            self.hist[\"train_prec\"].append(train_prec); self.hist[\"val_prec\"].append(val_prec); self.hist[\"test_prec\"].append(test_prec)\n            self.hist[\"train_f1\"].append(train_f1);     self.hist[\"val_f1\"].append(val_f1);     self.hist[\"test_f1\"].append(test_f1)\n            self.hist[\"train_loss\"].append(train_loss); self.hist[\"val_loss\"].append(val_loss); self.hist[\"test_loss\"].append(test_loss)\n            self.hist[\"reward\"].append(reward)\n            self.hist[\"action\"].append(opt_idx)\n            self.hist[\"time\"].append(elapsed)\n\n            # 8) In log thông tin epoch ra màn hình\n            print(f\"[Ep {ep+1:03}] ValAcc={val_acc:.4f} | ValF1={val_f1:.4f} | \"\n                  f\"Opt={opt_name.upper():4} | Δt={elapsed:.1f}s\")\n\n            # 9) Cập nhật các biến trạng thái cho epoch tiếp theo\n            prev_val_acc  = val_acc\n            prev_val_loss = val_loss\n            delta_acc_prev  = delta_acc\n            delta_loss_prev = delta_loss\n            prev_action = opt_idx\n\n            # Kiểm tra dừng sớm (early stopping)\n            if patience_counter >= self.patience:\n                print(f\"⏹ Early stop at epoch {ep + 1}\")\n                done = True\n                break  # kết thúc sớm nếu không cải thiện trong self.patience epoch liên tiếp\n\n        # Nếu dừng sớm trước n_epochs, thực hiện cập nhật PPO lần cuối cho các bước còn lại trong buffer\n        if ep < self.n_epochs - 1:\n            if len(buffer.states) > 0:\n                buffer.compute(torch.tensor(0.0, device=DEVICE))\n                self._ppo_update(buffer)\n                buffer.clear()\n\n# ==== 10) Sử dụng các hàm/lớp trên để huấn luyện mô hình với PPO ====\n# 1. Tải dữ liệu\ndl_train, dl_val, dl_test = load_fashion_mnist()\n# 2. Tạo mô hình ResNet-18 mới (train từ đầu)\nmodel = get_resnet18_scratch(num_classes=10)\n# 3. Khởi tạo trainer và bắt đầu huấn luyện\ntrainer = OptimizerRLTrainer(model, dl_train, dl_val, dl_test, n_epochs=150, patience=15)\ntrainer.fit()\n\n# 4. Xuất bảng reward và bảng chỉ số đầy đủ sau huấn luyện\nreward_df = pd.DataFrame({\n    \"Epoch\": range(1, len(trainer.hist[\"reward\"]) + 1),\n    \"Reward\": trainer.hist[\"reward\"],\n})\nprint(\"\\n===== REWARD TABLE =====\")\nprint(reward_df.to_string(index=False))\n# ----- BEST-VAL summary -------------------------------------------------\nbest_idx = int(np.argmax(trainer.hist[\"val_acc\"]))   # vị trí epoch tốt nhất\nprint(\"\\n===== BEST (Val Acc) =====\")\nprint(f\"Epoch {best_idx+1} | \"\n      f\"ValAcc {trainer.hist['val_acc'][best_idx]:.4f} | \"\n      f\"TestAcc {trainer.hist['test_acc'][best_idx]:.4f} | \"\n      f\"TestLoss {trainer.hist['test_loss'][best_idx]:.4f}\")\n\nfull_df = pd.DataFrame(trainer.hist).rename(columns={\"time\": \"Time(s)\"})\nfull_df.index = full_df.index + 1  # đặt index bắt đầu từ 1 (tương ứng số epoch)\nfull_df.index.name = \"Epoch\"\nprint(\"\\n===== FULL METRICS TABLE =====\")\nprint(full_df.to_string())\n\n# 5. Vẽ các biểu đồ theo yêu cầu (reward, accuracy, loss, precision, F1, time)\nepochs = range(1, len(trainer.hist[\"reward\"]) + 1)\n\n# Biểu đồ reward mỗi epoch\nplt.figure(figsize=(8, 4))\nplt.plot(reward_df[\"Epoch\"], reward_df[\"Reward\"])\nplt.title(\"Reward per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Reward\")\nplt.grid(True)\nplt.show()\n\n# Biểu đồ Accuracy (Train/Val/Test)\nplt.figure(figsize=(8, 4))\nplt.plot(epochs, trainer.hist[\"train_acc\"], label=\"Train\")\nplt.plot(epochs, trainer.hist[\"val_acc\"],   label=\"Val\")\nplt.plot(epochs, trainer.hist[\"test_acc\"],  label=\"Test\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\nplt.legend(); plt.grid(True)\nplt.show()\n\n# Biểu đồ Precision (Train/Val/Test)\nplt.figure(figsize=(8, 4))\nplt.plot(epochs, trainer.hist[\"train_prec\"], label=\"Train\")\nplt.plot(epochs, trainer.hist[\"val_prec\"],   label=\"Val\")\nplt.plot(epochs, trainer.hist[\"test_prec\"],  label=\"Test\")\nplt.title(\"Precision\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Precision\")\nplt.legend(); plt.grid(True)\nplt.show()\n\n# Biểu đồ F1-score (Train/Val/Test)\nplt.figure(figsize=(8, 4))\nplt.plot(epochs, trainer.hist[\"train_f1\"], label=\"Train\")\nplt.plot(epochs, trainer.hist[\"val_f1\"],   label=\"Val\")\nplt.plot(epochs, trainer.hist[\"test_f1\"],  label=\"Test\")\nplt.title(\"F1-Score\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"F1-Score\")\nplt.legend(); plt.grid(True)\nplt.show()\n\n# Biểu đồ Loss (Train/Val/Test)\nplt.figure(figsize=(8, 4))\nplt.plot(epochs, trainer.hist[\"train_loss\"], label=\"Train\")\nplt.plot(epochs, trainer.hist[\"val_loss\"],   label=\"Val\")\nplt.plot(epochs, trainer.hist[\"test_loss\"],  label=\"Test\")\nplt.title(\"Loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\nplt.legend(); plt.grid(True)\nplt.show()\n\n# Biểu đồ thời gian (độ trễ mỗi epoch)\nplt.figure(figsize=(8, 4))\nplt.plot(epochs, trainer.hist[\"time\"])\nplt.title(\"Elapsed Time per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Seconds\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:09:04.065968Z","iopub.execute_input":"2025-07-03T18:09:04.066312Z"}},"outputs":[{"name":"stdout","text":"Dataset FashionMNIST → Train: 55000 | Val: 5000 | Test: 10000\n[Ep 001] ValAcc=0.7804 | ValF1=0.7774 | Opt=SGD  | Δt=20.7s\n[Ep 002] ValAcc=0.8266 | ValF1=0.8268 | Opt=SGD  | Δt=20.4s\n[Ep 003] ValAcc=0.8344 | ValF1=0.8373 | Opt=SGD  | Δt=20.7s\n[Ep 004] ValAcc=0.8338 | ValF1=0.8347 | Opt=SGD  | Δt=20.8s\n[Ep 005] ValAcc=0.8584 | ValF1=0.8595 | Opt=SAM  | Δt=29.2s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"","metadata":{}}]}