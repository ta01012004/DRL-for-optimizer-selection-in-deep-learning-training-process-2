{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102},{"sourceId":714968,"sourceType":"datasetVersion","datasetId":366471},{"sourceId":3032274,"sourceType":"datasetVersion","datasetId":1856924}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.optim import Optimizer\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:02:22.013902Z","iopub.execute_input":"2025-05-25T07:02:22.014379Z","iopub.status.idle":"2025-05-25T07:02:36.422937Z","shell.execute_reply.started":"2025-05-25T07:02:22.01435Z","shell.execute_reply":"2025-05-25T07:02:36.421951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. SAM Optimizer Implementation\nclass SAM(Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        if rho < 0.0:\n            raise ValueError(f\"Invalid rho: {rho}\")\n        defaults = dict(rho=rho, **kwargs)\n        super().__init__(params, defaults)\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.eps = 1e-12\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        scale = self.param_groups[0]['rho'] / (grad_norm + self.eps)\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                e_w = p.grad * scale\n                p.add_(e_w)\n                self.state[p]['e_w'] = e_w\n        if zero_grad:\n            self.zero_grad()\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                p.sub_(self.state[p]['e_w'])\n        self.base_optimizer.step()\n        if zero_grad:\n            self.zero_grad()\n\n    def step(self, closure=None):\n        assert closure is not None, \"SAM requires closure\"\n        closure().backward()\n        self.first_step(zero_grad=True)\n        closure().backward()\n        self.second_step(zero_grad=True)\n\n    def _grad_norm(self):\n        device = self.param_groups[0]['params'][0].device\n        norms = [p.grad.norm(p=2).to(device)\n                 for group in self.param_groups for p in group['params'] if p.grad is not None]\n        return torch.norm(torch.stack(norms), p=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:02:36.424303Z","iopub.execute_input":"2025-05-25T07:02:36.424666Z","iopub.status.idle":"2025-05-25T07:02:36.436411Z","shell.execute_reply.started":"2025-05-25T07:02:36.424642Z","shell.execute_reply":"2025-05-25T07:02:36.434027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\nimport heapq\n\n# --- 1. Prioritized Replay Memory ---\nclass PrioritizedReplayMemory:\n    def __init__(self, capacity, alpha=0.6):\n        self.capacity = capacity\n        self.alpha = alpha\n        self.buffer = []\n        self.priorities = []\n        self.pos = 0\n\n    def add(self, experience, error):\n        priority = (abs(error) + 1e-5) ** self.alpha\n        if len(self.buffer) < self.capacity:\n            self.buffer.append(experience)\n            self.priorities.append(priority)\n        else:\n            self.buffer[self.pos] = experience\n            self.priorities[self.pos] = priority\n        self.pos = (self.pos + 1) % self.capacity\n\n    def sample(self, batch_size, beta=0.4):\n        probs = np.array(self.priorities) ** self.alpha\n        probs /= probs.sum()\n        \n        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n        samples = [self.buffer[idx] for idx in indices]\n        \n        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n        weights /= weights.max()\n        \n        return samples, indices, weights\n\n    def update_priority(self, idx, error):\n        self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha\n\n# --- 2. Modified DQN Agent ---\nclass DQNAgent:\n    def __init__(self, state_dim=512, action_dim=10,  \n                 gamma=0.99, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n        self.gamma = gamma\n        self.epsilon = epsilon_start\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.memory = PrioritizedReplayMemory(10000)\n        self.learn_step = 0\n        self.action_dim = action_dim  # Thêm thuộc tính action_dim\n\n        # Q-networks\n        layers = [nn.Linear(state_dim, 64), nn.ReLU(),\n                  nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, action_dim)]  # Sử dụng action_dim\n        self.policy_net = nn.Sequential(*layers)\n        self.target_net = nn.Sequential(*layers)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n        self.model = self.policy_net  # Alias\n\n    def act(self, state):\n        if random.random() < self.epsilon:\n            return random.randrange(self.action_dim)  # Sử dụng self.action_dim\n        with torch.no_grad():\n            q = self.policy_net(torch.FloatTensor(state).unsqueeze(0))\n        return int(q.argmax(1).item())\n\n    # Giữ nguyên các hàm remember và learn từ code của bạn\n\n# --- 3. Integration với ResNet và SAM ---\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, pretrained: bool = False):      # ➊  mặc định = False\n        super().__init__()\n\n        # ➋  Nếu KHÔNG muốn pre-train → weights=None\n        backbone = models.resnet18(\n            weights=(\n                models.ResNet18_Weights.DEFAULT\n                if pretrained else None               #  ← trọng số ngẫu nhiên\n            )\n        )\n\n        # ➌  Bỏ lớp FC và lưu feature_dim\n        self.features = nn.Sequential(*list(backbone.children())[:-1])\n        self.feature_dim = backbone.fc.in_features     # =512\n\n    def forward(self, x):\n        x = self.features(x)          # [B, 512, 1, 1]\n        return x.view(x.size(0), -1)  # [B, 512]\n\n\ndef initialize_system():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Khởi tạo ResNet với feature extractor\n    resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n    feature_extractor = FeatureExtractor(resnet).to(device)\n    \n    # Khởi tạo DQN Agent\n    dqn_agent = DQNAgent(\n        state_dim=feature_extractor.feature_dim,\n        action_dim=10,  # Số lớp trong CIFAR-10\n        gamma=0.99,\n        epsilon_start=1.0,\n        epsilon_min=0.01,\n        epsilon_decay=0.995\n    ).to(device)\n    \n    # Khởi tạo SAM Optimizer cho ResNet\n    base_optimizer = optim.SGD\n    sam_optimizer = SAM(\n        feature_extractor.parameters(),\n        base_optimizer,\n        lr=0.1,\n        momentum=0.9\n    )\n    \n    return feature_extractor, dqn_agent, sam_optimizer, device\n\n# --- 4. Training Loop với tích hợp DQN ---\ndef train_epoch(feature_extractor, dqn_agent, sam_optimizer, train_loader, criterion, device):\n    feature_extractor.train()\n    \n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # SAM first step\n        features = feature_extractor(inputs)\n        loss = criterion(features, targets)  # Giả sử có classifier phù hợp\n        loss.backward()\n        sam_optimizer.first_step(zero_grad=True)\n        \n        # SAM second step\n        criterion(feature_extractor(inputs), targets).backward()\n        sam_optimizer.second_step(zero_grad=True)\n        \n        # DQN Experience Collection\n        state = features[0].detach().cpu().numpy()\n        action = dqn_agent.act(state)\n        next_state = feature_extractor(inputs)[0].detach().cpu().numpy()\n        reward = 1.0 if torch.argmax(features[0]) == targets[0] else -1.0\n        done = batch_idx == len(train_loader) - 1\n        \n        dqn_agent.remember(state, action, reward, next_state, done)\n        dqn_agent.learn()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:05:33.70887Z","iopub.status.idle":"2025-05-25T07:05:33.709133Z","shell.execute_reply.started":"2025-05-25T07:05:33.709013Z","shell.execute_reply":"2025-05-25T07:05:33.709033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split  # <-- thêm random_split vào đây\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:02:36.471987Z","iopub.execute_input":"2025-05-25T07:02:36.472897Z","iopub.status.idle":"2025-05-25T07:02:36.496878Z","shell.execute_reply.started":"2025-05-25T07:02:36.47286Z","shell.execute_reply":"2025-05-25T07:02:36.496048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Model Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\nopt_sgd = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\nopt_sam = SAM(model.parameters(), optim.SGD, lr=0.1, momentum=0.9, rho=0.05)\nagent = DQNAgent(state_dim=2, action_dim=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:02:36.497612Z","iopub.execute_input":"2025-05-25T07:02:36.497936Z","iopub.status.idle":"2025-05-25T07:02:37.127727Z","shell.execute_reply.started":"2025-05-25T07:02:36.497906Z","shell.execute_reply":"2025-05-25T07:02:37.126853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Định nghĩa transforms cho tập huấn luyện và tập validation\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n                         std=(0.2023, 0.1994, 0.2010))\n])\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n                         std=(0.2023, 0.1994, 0.2010))\n])\n\n# Tạo dataset từ thư mục train (CIFAR-10 training set)\ntrain_dir = \"/kaggle/input/cifar10/cifar10/train\"\nfull_dataset = datasets.ImageFolder(root=train_dir)\n\n# Chia dataset thành 90% train và 10% val\ndataset_size = len(full_dataset)\ntrain_size = int(0.9 * dataset_size)\nval_size = dataset_size - train_size\ntrain_subset, val_subset = random_split(full_dataset, [train_size, val_size])\ntrain_indices = train_subset.indices\nval_indices = val_subset.indices\n\n# Tạo dataset cho train và val với các transform đã định nghĩa\ntrain_dataset = torch.utils.data.Subset(\n    datasets.ImageFolder(root=train_dir, transform=train_transform),\n    train_indices\n)\nval_dataset = torch.utils.data.Subset(\n    datasets.ImageFolder(root=train_dir, transform=val_transform),\n    val_indices\n)\n\n# Tạo DataLoader cho tập train và val\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n                          num_workers=0, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False,\n                        num_workers=0, pin_memory=True)\n\n# In số lượng mẫu trong mỗi tập\nprint(f\"Số lượng ảnh train: {len(train_dataset)}\")\nprint(f\"Số lượng ảnh val: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:02:37.128901Z","iopub.execute_input":"2025-05-25T07:02:37.129215Z","iopub.status.idle":"2025-05-25T07:05:32.980341Z","shell.execute_reply.started":"2025-05-25T07:02:37.12919Z","shell.execute_reply":"2025-05-25T07:05:32.979413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Training & Validation Functions\ndef train_one_epoch(model, optimizer, loader, device):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for imgs, lbls in loader:\n        imgs, lbls = imgs.to(device), lbls.to(device)\n        # Sharpness-Aware Minimization (SAM) requires closure\n        if isinstance(optimizer, SAM):\n            # define closure for SAM\n            def closure():\n                optimizer.zero_grad()\n                out = model(imgs)\n                loss = criterion(out, lbls)\n                return loss\n            # first forward-backward and SAM step\n            loss = closure()\n            optimizer.step(closure)\n            # recompute outputs for stats\n            with torch.no_grad():\n                out = model(imgs)\n        else:\n            optimizer.zero_grad()\n            out = model(imgs)\n            loss = criterion(out, lbls)\n            loss.backward()\n            optimizer.step()\n        # accumulate metrics\n        total_loss += loss.item() * lbls.size(0)\n        preds = out.argmax(1)\n        correct += (preds == lbls).sum().item()\n        total += lbls.size(0)\n    return total_loss/total, 100 * correct/total\n\n\ndef validate(model, loader, device):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for imgs, lbls in loader:\n            imgs, lbls = imgs.to(device), lbls.to(device)\n            out = model(imgs)\n            correct += (out.argmax(1) == lbls).sum().item()\n            total += lbls.size(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:05:32.981254Z","iopub.execute_input":"2025-05-25T07:05:32.981987Z","iopub.status.idle":"2025-05-25T07:05:32.990913Z","shell.execute_reply.started":"2025-05-25T07:05:32.981963Z","shell.execute_reply":"2025-05-25T07:05:32.990077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, torch.nn as nn, torch.optim as optim\nimport numpy as np, random\nfrom collections import deque\n\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim,\n                 gamma=0.99, epsilon=1.0,\n                 epsilon_min=0.01, epsilon_decay=0.995):\n        self.state_dim     = state_dim\n        self.action_dim    = action_dim\n        self.gamma         = gamma\n        self.epsilon       = epsilon\n        self.epsilon_min   = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.memory        = deque(maxlen=10_000)\n\n        self.policy_net = nn.Sequential(\n            nn.Linear(state_dim, 256), nn.ReLU(),\n            nn.Linear(256, 128),       nn.ReLU(),\n            nn.Linear(128, action_dim)\n        )\n        self.target_net  = nn.Sequential(*[layer for layer in self.policy_net])\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.optimizer   = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n        self.learn_step  = 0\n\n    # ---- hành động ε-greedy -----------------------------------------------\n    def act(self, state):\n        if random.random() < self.epsilon:\n            return random.randint(0, self.action_dim - 1)\n        state = torch.as_tensor(state, dtype=torch.float32).unsqueeze(0)\n        with torch.no_grad():\n            q = self.policy_net(state)\n        return q.argmax(1).item()\n\n    # ---- lưu trải nghiệm ----------------------------------------------------\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    # ---- học ----------------------------------------------------------------\n    def learn(self, batch_size=32):\n        if len(self.memory) < batch_size:\n            return 0.0\n\n        batch      = random.sample(self.memory, batch_size)\n        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n\n        states      = torch.as_tensor(states,      dtype=torch.float32)\n        next_states = torch.as_tensor(next_states, dtype=torch.float32)\n        actions     = torch.as_tensor(actions,     dtype=torch.long)\n        rewards     = torch.as_tensor(rewards,     dtype=torch.float32)\n        dones       = torch.as_tensor(dones,       dtype=torch.float32)\n\n        q_pred = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            q_next = self.target_net(next_states).max(1)[0]\n            q_trg  = rewards + self.gamma * q_next * (1 - dones)\n\n        loss = nn.functional.mse_loss(q_pred, q_trg)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        if self.learn_step % 100 == 0:\n            self.target_net.load_state_dict(self.policy_net.state_dict())\n\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n        self.learn_step += 1\n        return loss.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:22:42.019123Z","iopub.execute_input":"2025-05-25T07:22:42.019517Z","iopub.status.idle":"2025-05-25T07:22:42.033355Z","shell.execute_reply.started":"2025-05-25T07:22:42.019484Z","shell.execute_reply":"2025-05-25T07:22:42.032578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, feature_dim: int, num_classes: int):\n        super().__init__()\n        self.fc = nn.Linear(feature_dim, num_classes)\n\n    def forward(self, x):\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:19:47.887182Z","iopub.execute_input":"2025-05-25T07:19:47.887987Z","iopub.status.idle":"2025-05-25T07:19:47.892485Z","shell.execute_reply.started":"2025-05-25T07:19:47.88796Z","shell.execute_reply":"2025-05-25T07:19:47.891708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ndef prepare_data(\n    train_batch_size: int = 128,\n    val_batch_size: int = 256,\n    num_workers: int = 2,\n    pin_memory: bool = True\n):\n    \"\"\"\n    Tải CIFAR-10, áp dụng transform và trả về train_loader, val_loader.\n    train_batch_size: batch size cho training (mặc định 128)\n    val_batch_size: batch size cho validation/test (mặc định 256)\n    num_workers: số worker cho DataLoader\n    pin_memory: nếu True sẽ pin_memory để tối ưu copy lên GPU\n    \"\"\"\n    # Transforms\n    train_tf = transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225))\n    ])\n    val_tf = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225))\n    ])\n\n    # Datasets\n    train_set = datasets.CIFAR10(\n        root=\"./data\", train=True, download=True, transform=train_tf\n    )\n    val_set = datasets.CIFAR10(\n        root=\"./data\", train=False, download=True, transform=val_tf\n    )\n\n    # DataLoaders\n    train_loader = DataLoader(\n        train_set,\n        batch_size=train_batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=val_batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory\n    )\n\n    return train_loader, val_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T09:02:06.297202Z","iopub.execute_input":"2025-05-25T09:02:06.297518Z","iopub.status.idle":"2025-05-25T09:02:06.305499Z","shell.execute_reply.started":"2025-05-25T09:02:06.297492Z","shell.execute_reply":"2025-05-25T09:02:06.304848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import annotations\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Giả sử đã import:\n# from feature_extractor import FeatureExtractor\n# from classifier       import Classifier\n# from dqn_agent        import DQNAgent\n\n# Import DataLoader và dataset trực tiếp\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import Compose, ToTensor, Resize\n\n# Cấu hình hệ số phạt thời gian cho reward\nW_TIME = 0.05\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # -------- 1. Mô hình ----------\n    feature_extractor = FeatureExtractor(pretrained=False).to(device)\n    classifier        = Classifier(feature_dim=512, num_classes=10).to(device)\n    params = list(feature_extractor.parameters()) + list(classifier.parameters())\n\n    # -------- 2. Hai optimizer tách biệt ----------\n    opt_sgd = torch.optim.SGD(params, lr=0.1, momentum=0.9)\n    opt_sam = SAM(params, torch.optim.SGD, lr=0.1, momentum=0.9)\n\n    criterion = nn.CrossEntropyLoss()\n    dqn_agent = DQNAgent(state_dim=512, action_dim=2)  # 0:SGD, 1:SAM\n\n    # -------- 3. Chuẩn bị dữ liệu CIFAR-10 --------\n    transform = Compose([Resize((32, 32)), ToTensor()])\n    train_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n    val_dataset   = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=128,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    test_loader = DataLoader(\n        val_dataset,\n        batch_size=256,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    NUM_EPOCHS, best_acc = 100, 0.0\n    results = {k: [] for k in [\n        \"reward_history\", \"train_loss_history\",\n        \"train_acc_history\", \"val_acc_history\", \"dqn_loss_history\"\n    ]}\n\n    for epoch in range(NUM_EPOCHS):\n        # ----- reset thống kê -----\n        epoch_start    = time.time()\n        total_reward   = 0.0\n        total_dqn_loss = 0.0\n        train_loss_sum = 0.0\n        train_correct  = 0\n        train_seen     = 0\n        feature_extractor.train(); classifier.train()\n\n        # Khởi tạo state và biến lưu action cuối\n        state = None\n        last_action_str = \"\"\n\n        # Duyệt qua từng batch\n        for batch_idx, (inputs, targets) in enumerate(train_loader, start=1):\n            batch_start = time.time()\n\n            inputs, targets = inputs.to(device), targets.to(device)\n            features = feature_extractor(inputs)  # [B,512]\n            outputs  = classifier(features)\n            loss     = criterion(outputs, targets)\n\n            # ---- Xây state ban đầu nếu chưa có ----\n            if state is None:\n                state = features[0].detach().cpu().numpy()\n\n            # =============== DQN chọn optimizer ===============\n            action = dqn_agent.act(state)  # 0→SGD, 1→SAM\n            last_action_str = \"SGD\" if action == 0 else \"SAM\"\n            print(\n                f\"Epoch {epoch+1:03}/{NUM_EPOCHS} - \"\n                f\"Batch {batch_idx:03}/{len(train_loader)} - \"\n                f\"Action: {last_action_str}\"\n            )\n\n            # Áp dụng optimiser\n            if action == 0:\n                opt_sgd.zero_grad()\n                loss.backward()\n                opt_sgd.step()\n            else:\n                loss.backward()\n                opt_sam.first_step(zero_grad=True)\n                outputs2 = classifier(feature_extractor(inputs))\n                criterion(outputs2, targets).backward()\n                opt_sam.second_step(zero_grad=True)\n\n            # Tính elapsed và reward\n            batch_elapsed = time.time() - batch_start\n            reward        = -loss.item() - W_TIME * batch_elapsed\n\n            # Cập nhật DQN\n            next_state = feature_extractor(inputs)[0].detach().cpu().numpy()\n            dqn_agent.remember(state, action, reward, next_state, False)\n            dqn_loss = dqn_agent.learn()\n\n            # Cập nhật state\n            state = next_state\n\n            # ----- cộng dồn -----\n            total_reward   += reward\n            total_dqn_loss += dqn_loss\n            train_loss_sum += loss.item() * targets.size(0)\n            train_correct  += (outputs.argmax(1) == targets).sum().item()\n            train_seen     += targets.size(0)\n\n        # ----- thống kê cuối epoch -----\n        train_loss = train_loss_sum / train_seen\n        train_acc  = 100. * train_correct / train_seen\n        val_acc    = validate(feature_extractor, classifier, test_loader, device)\n        epoch_elapsed = time.time() - epoch_start\n\n        # Lưu kết quả\n        results[\"reward_history\"].append(total_reward)\n        results[\"dqn_loss_history\"].append(total_dqn_loss / max(len(train_loader), 1))\n        results[\"train_loss_history\"].append(train_loss)\n        results[\"train_acc_history\"].append(train_acc)\n        results[\"val_acc_history\"].append(val_acc)\n\n        # In summary kèm optimiser cuối cùng\n        print(\n            f\"Epoch {epoch+1:03}/{NUM_EPOCHS} | \"\n            f\"LastOpt {last_action_str} | \"\n            f\"Reward {total_reward:8.1f} | \"\n            f\"TrainLoss {train_loss:.4f} | \"\n            f\"TrainAcc {train_acc:6.2f}% | \"\n            f\"ValAcc {val_acc:6.2f}% | \"\n            f\"DQN_Loss {results['dqn_loss_history'][-1]:.4f} | \"\n            f\"Time {epoch_elapsed:.1f}s\"\n        )\n\n        # Lưu model tốt nhất\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save({\n                \"feature_extractor\": feature_extractor.state_dict(),\n                \"classifier\": classifier.state_dict()\n            }, \"best_model.pth\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T09:04:35.253915Z","iopub.execute_input":"2025-05-25T09:04:35.254713Z","iopub.status.idle":"2025-05-25T09:37:41.437812Z","shell.execute_reply.started":"2025-05-25T09:04:35.254691Z","shell.execute_reply":"2025-05-25T09:37:41.436745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import annotations\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n# Giả sử đã import:\n# from feature_extractor import FeatureExtractor\n# from classifier       import Classifier\n# from dqn_agent        import DQNAgent\n# from data_utils       import validate, SAM\n\n# Import DataLoader và dataset trực tiếp\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import Compose, ToTensor, Resize\n\n# Import confusion matrix tools\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Cấu hình hệ số phạt thời gian cho reward\nW_TIME = 0.05\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # -------- 1. Mô hình ----------\n    feature_extractor = FeatureExtractor(pretrained=False).to(device)\n    classifier        = Classifier(feature_dim=512, num_classes=10).to(device)\n    params = list(feature_extractor.parameters()) + list(classifier.parameters())\n\n    # -------- 2. Hai optimizer tách biệt ----------\n    opt_sgd = torch.optim.SGD(params, lr=0.1, momentum=0.9)\n    opt_sam = SAM(params, torch.optim.SGD, lr=0.1, momentum=0.9)\n\n    criterion = nn.CrossEntropyLoss()\n    dqn_agent = DQNAgent(state_dim=512, action_dim=2)  # 0:SGD, 1:SAM\n\n    # -------- 3. Chuẩn bị dữ liệu CIFAR-10 --------\n    transform = Compose([Resize((32, 32)), ToTensor()])\n    train_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n    val_dataset   = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n    test_loader  = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\n    NUM_EPOCHS, best_acc = 100, 0.0\n    results = {\n        \"reward_history\": [],\n        \"train_loss_history\": [],\n        \"train_acc_history\": [],\n        \"val_acc_history\": [],\n        \"dqn_loss_history\": [],\n        \"time_history\": []\n    }\n\n    # Train loop\n    for epoch in range(NUM_EPOCHS):\n        epoch_start    = time.time()\n        total_reward   = 0.0\n        total_dqn_loss = 0.0\n        train_loss_sum = 0.0\n        train_correct  = 0\n        train_seen     = 0\n        feature_extractor.train(); classifier.train()\n\n        state = None\n        last_action_str = \"\"\n\n        for batch_idx, (inputs, targets) in enumerate(train_loader, start=1):\n            batch_start = time.time()\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            features = feature_extractor(inputs)\n            outputs  = classifier(features)\n            loss     = criterion(outputs, targets)\n\n            if state is None:\n                state = features[0].detach().cpu().numpy()\n\n            # Chọn optimizer và thực hiện bước cập nhật\n            action = dqn_agent.act(state)\n            last_action_str = \"SGD\" if action == 0 else \"SAM\"\n            if action == 0:\n                opt_sgd.zero_grad(); loss.backward(); opt_sgd.step()\n            else:\n                loss.backward(); opt_sam.first_step(zero_grad=True)\n                outputs2 = classifier(feature_extractor(inputs))\n                criterion(outputs2, targets).backward(); opt_sam.second_step(zero_grad=True)\n\n            batch_elapsed = time.time() - batch_start\n            reward        = -loss.item() - W_TIME * batch_elapsed\n\n            next_state = feature_extractor(inputs)[0].detach().cpu().numpy()\n            dqn_agent.remember(state, action, reward, next_state, False)\n            dqn_loss = dqn_agent.learn()\n            state = next_state\n\n            total_reward   += reward\n            total_dqn_loss += dqn_loss\n            train_loss_sum += loss.item() * targets.size(0)\n            train_correct  += (outputs.argmax(1) == targets).sum().item()\n            train_seen     += targets.size(0)\n\n        train_loss = train_loss_sum / train_seen\n        train_acc  = 100. * train_correct / train_seen\n        val_acc    = validate(feature_extractor, classifier, test_loader, device)\n        epoch_elapsed = time.time() - epoch_start\n\n                # Lưu lịch sử\n        results[\"reward_history\"].append(total_reward)\n        results[\"train_loss_history\"].append(train_loss)\n        results[\"train_acc_history\"].append(train_acc)\n        results[\"val_acc_history\"].append(val_acc)\n        results[\"dqn_loss_history\"].append(total_dqn_loss / max(len(train_loader), 1))\n        results[\"time_history\"].append(epoch_elapsed)\n\n        # In summary\n        print(f\"Epoch {epoch+1:03}/{NUM_EPOCHS} | LastOpt {last_action_str} | \"\n              f\"Reward {total_reward:.1f} | TrainLoss {train_loss:.4f} | \"\n              f\"TrainAcc {train_acc:.2f}% | ValAcc {val_acc:.2f}% | \"\n              f\"DQN_Loss {results['dqn_loss_history'][-1]:.4f} | Time {epoch_elapsed:.1f}s\")\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save({\n                \"feature_extractor\": feature_extractor.state_dict(),\n                \"classifier\": classifier.state_dict()\n            }, \"best_model.pth\")\n\n    # ---- Sau khi training xong, lưu kết quả ra file ----\n    import pickle\n    with open(\"results.pkl\", \"wb\") as f:\n        pickle.dump(results, f)\n    torch.save(feature_extractor.state_dict(), \"feat.pth\")\n    torch.save(classifier.state_dict(),       \"clf.pth\")\n\n    return results, feature_extractor, classifier, device, test_loader\n\n\n# ==== Code cell: Plotting & Confusion Matrix (reuse variables) ====  \n# Đảm bảo biến đã có sẵn, nếu chưa thì gọi main() để huấn luyện và lấy kết quả\ntry:\n    results\nexcept NameError:\n    results, feature_extractor, classifier, device, test_loader = main()\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport torch\n\n# 1️⃣ Vẽ biểu đồ tổng hợp\nepochs = range(1, len(results[\"reward_history\"]) + 1)\nplt.figure(figsize=(14, 10))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, results[\"reward_history\"], label=\"Reward per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Reward\"); plt.title(\"Reward per Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, results[\"train_loss_history\"], label=\"Train Loss per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Train Loss per Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, results[\"train_acc_history\"], label=\"Train Accuracy per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Train Accuracy per Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 4)\nplt.plot(results[\"time_history\"], results[\"dqn_loss_history\"], label=\"DQN Loss vs Time\")\nplt.xlabel(\"Time (s)\"); plt.ylabel(\"DQN Loss\"); plt.title(\"DQN Loss vs Time\"); plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 2️⃣ In ma trận nhầm lẫn\nfeature_extractor.eval()\nclassifier.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = classifier(feature_extractor(inputs))\n        all_preds.append(outputs.argmax(dim=1).cpu())\n        all_labels.append(labels.cpu())\nall_preds  = torch.cat(all_preds).numpy()\nall_labels = torch.cat(all_labels).numpy()\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:10:27.885302Z","iopub.execute_input":"2025-05-25T10:10:27.885575Z","iopub.status.idle":"2025-05-25T10:45:05.831427Z","shell.execute_reply.started":"2025-05-25T10:10:27.885558Z","shell.execute_reply":"2025-05-25T10:45:05.830513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1️⃣ Vẽ biểu đồ tổng hợp\nepochs = range(1, len(results[\"reward_history\"]) + 1)\nplt.figure(figsize=(14, 10))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, results[\"reward_history\"], label=\"Reward per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Reward\"); plt.title(\"Reward per Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, results[\"train_loss_history\"], label=\"Train Loss per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Train Loss per Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, results[\"train_acc_history\"], label=\"Train Accuracy per Epoch\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Train Accuracy per Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 4)\nplt.plot(results[\"time_history\"], results[\"dqn_loss_history\"], label=\"DQN Loss vs Time\")\nplt.xlabel(\"Time (s)\"); plt.ylabel(\"DQN Loss\"); plt.title(\"DQN Loss vs Time\"); plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 2️⃣ In ma trận nhầm lẫn\nfeature_extractor.eval()\nclassifier.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = classifier(feature_extractor(inputs))\n        all_preds.append(outputs.argmax(dim=1).cpu())\n        all_labels.append(labels.cpu())\nall_preds  = torch.cat(all_preds).numpy()\nall_labels = torch.cat(all_labels).numpy()\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:42:43.082913Z","iopub.execute_input":"2025-05-25T13:42:43.083582Z","iopub.status.idle":"2025-05-25T13:42:43.152309Z","shell.execute_reply.started":"2025-05-25T13:42:43.083554Z","shell.execute_reply":"2025-05-25T13:42:43.151507Z"}},"outputs":[],"execution_count":null}]}